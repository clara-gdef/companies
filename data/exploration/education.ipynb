{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import ipdb\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_file = \"bp_3jobs_desc_edu_skills_industry_date_company_FR.json\"\n",
    "MIN_JOB_COUNT = 3\n",
    "MAX_SEQ_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "global CFG\n",
    "with open(\"../../config.yaml\", \"r\") as ymlfile:\n",
    "    CFG = yaml.load(ymlfile, Loader=yaml.SafeLoader)\n",
    "# with ipdb.launch_ipdb_on_exception():\n",
    "cie_file = os.path.join(CFG[\"datadir\"], \"cie_list.pkl\")\n",
    "with open(cie_file, \"rb\") as f:\n",
    "    cie_list = pkl.load(f)\n",
    "synonym_file = os.path.join(CFG[\"datadir\"], \"cie_synonyms.pkl\")\n",
    "with open(synonym_file, \"rb\") as f:\n",
    "    syn_cie = pkl.load(f)\n",
    "blacklist_file = os.path.join(CFG[\"datadir\"], \"blacklist.pkl\")\n",
    "with open(blacklist_file, \"rb\") as f:\n",
    "    blacklist = pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_seq_into_list(position, description, cie_list, syn_cie):\n",
    "    number_regex = re.compile(r'\\d+(,\\d+)?')\n",
    "    whole_job = position.lower() + ' ' + description.lower()\n",
    "    new_tup = []\n",
    "\n",
    "    for cie in cie_list:\n",
    "        if cie in whole_job.lower():\n",
    "            if cie in syn_cie.keys():\n",
    "                handle = syn_cie[cie]\n",
    "            else:\n",
    "                handle = cie\n",
    "            whole_job = whole_job.replace(cie, handle)\n",
    "\n",
    "    for name in syn_cie.keys():\n",
    "        if name in whole_job.lower():\n",
    "            handle = syn_cie[name]\n",
    "            whole_job = whole_job.replace(cie, handle)\n",
    "\n",
    "    job = word_tokenize(whole_job)\n",
    "\n",
    "    for tok in job:\n",
    "        if re.match(number_regex, tok):\n",
    "            new_tup.append(\"NUM\")\n",
    "        elif tok.lower() in cie_list or tok.lower() in syn_cie.keys():\n",
    "            new_tup.append(\"CIE\")\n",
    "        else:\n",
    "            new_tup.append(tok.lower())\n",
    "    cleaned_tup = [item for item in new_tup if item != \"\"]\n",
    "    return cleaned_tup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_date(job):\n",
    "    if job[\"to\"] == \"Present\":\n",
    "        date_time_str = '2018-04-12'  # date of files creation\n",
    "        time = datetime.timestamp(datetime.strptime(date_time_str, '%Y-%m-%d'))\n",
    "    elif len(job[\"to\"].split(\" \")) == 2:\n",
    "        try:\n",
    "            time = datetime.timestamp(datetime.strptime(job[\"to\"], \"%B %Y\"))\n",
    "        except ValueError:\n",
    "            time = datetime.timestamp(datetime.strptime(job[\"to\"].split(\" \")[-1], \"%Y\"))\n",
    "    else:\n",
    "        try:\n",
    "            time = datetime.timestamp(datetime.strptime(job[\"to\"].split(\" \")[-1], \"%Y\"))\n",
    "        except ValueError:\n",
    "            date_time_str = '2018-04-13'  # date of files creation\n",
    "            time = datetime.timestamp(datetime.strptime(date_time_str, '%Y-%m-%d'))\n",
    "    tstmp = pd.Timestamp.fromtimestamp(time)\n",
    "    return round(datetime.timestamp(tstmp.round(\"D\").to_pydatetime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edu_info(person, cie_list, syn_cie, blacklist):\n",
    "    education = person[-2]\n",
    "    jobs = []\n",
    "    flag = False\n",
    "    for job in person[1]:\n",
    "        if 'company' in job.keys():\n",
    "            threshold = min(len(job[\"company\"].split(\" \")), 5)\n",
    "            tmp = job[\"company\"].split(\" \")[:threshold]\n",
    "            normalized_name = [unidecode.unidecode(name.lower()) for name in tmp]\n",
    "            company_name = \"\".join(normalized_name)\n",
    "            if company_name in cie_list:\n",
    "                flag = True\n",
    "    if flag:\n",
    "        for job in person[1]:\n",
    "            if 'company' in job.keys():\n",
    "                threshold = min(len(job[\"company\"].split(\" \")), 5)\n",
    "                tmp = job[\"company\"].split(\" \")[:threshold]\n",
    "                normalized_name = [unidecode.unidecode(name.lower()) for name in tmp]\n",
    "                company_name = \"\".join(normalized_name)\n",
    "                if company_name not in blacklist:\n",
    "                    end = handle_date(job)\n",
    "                    tstmp = pd.Timestamp.fromtimestamp(job[\"from_ts\"])\n",
    "                    start = round(datetime.timestamp(tstmp.round(\"D\").to_pydatetime()))\n",
    "                    if company_name in syn_cie.keys():\n",
    "                        cie = syn_cie[company_name]\n",
    "                    else:\n",
    "                        cie = company_name\n",
    "                    if (end > 0) and (start > 0):  # corresponds to the timestamp of 01/01/1970\n",
    "                        j = {'from': start,\n",
    "                             'to': end,\n",
    "                             'company': cie,\n",
    "                             'job': word_seq_into_list(job[\"position\"],\n",
    "                                                       job[\"description\"], cie_list,  syn_cie)}\n",
    "                        jobs.append(j)\n",
    "\n",
    "    return education, jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 710830/850902 [35:58<06:59, 333.79it/s] "
     ]
    }
   ],
   "source": [
    "current_file = os.path.join(CFG[\"prevdatadir\"], base_file)\n",
    "with open(current_file, 'r') as f:\n",
    "    num_lines = sum(1 for line in f)\n",
    "with open(current_file, 'r') as f:\n",
    "    pbar = tqdm(f, total=num_lines)\n",
    "    edu_backgrounds = []\n",
    "    for line in pbar:\n",
    "        current_person = json.loads(line)\n",
    "        jobs = current_person[1]\n",
    "        skills = current_person[2]\n",
    "        if len(jobs) >= MIN_JOB_COUNT and len(skills) > 0:\n",
    "            edu_info, new_jobs = get_edu_info(current_person, cie_list, syn_cie, blacklist)\n",
    "            if len(new_jobs) >= MIN_JOB_COUNT:\n",
    "                edu_backgrounds.extend(edu_info)\n",
    "        pbar.update(1)\n",
    "tgt_file = \"unprocessed_educations.pkl\"\n",
    "with open(os.path.join(CFG[\"datadir\"], tgt_file), \"wb\") as f:\n",
    "    pkl.dump(edu_backgrounds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_file = \"unprocessed_educations.pkl\"\n",
    "with open(os.path.join(CFG[\"datadir\"], tgt_file), \"wb\") as f:\n",
    "    pkl.dump(edu_backgrounds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/local/gainondefor/work/lip6/data/companies/unprocessed_educations.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-1343f8c7dc1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtgt_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"unprocessed_educations.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"datadir\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/local/gainondefor/work/lip6/data/companies/unprocessed_educations.pkl'"
     ]
    }
   ],
   "source": [
    "tgt_file = \"unprocessed_educations.pkl\"\n",
    "with open(os.path.join(CFG[\"datadir\"], tgt_file), \"rb\") as f:\n",
    "    data = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda10",
   "language": "python",
   "name": "cuda10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
