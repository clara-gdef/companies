{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "import scipy.stats as stats\n",
    "import ipdb\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_file = \"bp_3jobs_desc_edu_skills_industry_date_company_FR.json\"\n",
    "MIN_JOB_COUNT = 3\n",
    "MAX_SEQ_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global CFG\n",
    "with open(\"../../config.yaml\", \"r\") as ymlfile:\n",
    "    CFG = yaml.load(ymlfile, Loader=yaml.SafeLoader)\n",
    "# with ipdb.launch_ipdb_on_exception():\n",
    "cie_file = os.path.join(CFG[\"datadir\"], \"cie_list.pkl\")\n",
    "with open(cie_file, \"rb\") as f:\n",
    "    cie_list = pkl.load(f)\n",
    "synonym_file = os.path.join(CFG[\"datadir\"], \"cie_synonyms.pkl\")\n",
    "with open(synonym_file, \"rb\") as f:\n",
    "    syn_cie = pkl.load(f)\n",
    "blacklist_file = os.path.join(CFG[\"datadir\"], \"blacklist.pkl\")\n",
    "with open(blacklist_file, \"rb\") as f:\n",
    "    blacklist = pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_seq_into_list(position, description, cie_list, syn_cie):\n",
    "    number_regex = re.compile(r'\\d+(,\\d+)?')\n",
    "    whole_job = position.lower() + ' ' + description.lower()\n",
    "    new_tup = []\n",
    "\n",
    "    for cie in cie_list:\n",
    "        if cie in whole_job.lower():\n",
    "            if cie in syn_cie.keys():\n",
    "                handle = syn_cie[cie]\n",
    "            else:\n",
    "                handle = cie\n",
    "            whole_job = whole_job.replace(cie, handle)\n",
    "\n",
    "    for name in syn_cie.keys():\n",
    "        if name in whole_job.lower():\n",
    "            handle = syn_cie[name]\n",
    "            whole_job = whole_job.replace(cie, handle)\n",
    "\n",
    "    job = word_tokenize(whole_job)\n",
    "\n",
    "    for tok in job:\n",
    "        if re.match(number_regex, tok):\n",
    "            new_tup.append(\"NUM\")\n",
    "        elif tok.lower() in cie_list or tok.lower() in syn_cie.keys():\n",
    "            new_tup.append(\"CIE\")\n",
    "        else:\n",
    "            new_tup.append(tok.lower())\n",
    "    cleaned_tup = [item for item in new_tup if item != \"\"]\n",
    "    return cleaned_tup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_date(job):\n",
    "    if job[\"to\"] == \"Present\":\n",
    "        date_time_str = '2018-04-12'  # date of files creation\n",
    "        time = datetime.timestamp(datetime.strptime(date_time_str, '%Y-%m-%d'))\n",
    "    elif len(job[\"to\"].split(\" \")) == 2:\n",
    "        try:\n",
    "            time = datetime.timestamp(datetime.strptime(job[\"to\"], \"%B %Y\"))\n",
    "        except ValueError:\n",
    "            time = datetime.timestamp(datetime.strptime(job[\"to\"].split(\" \")[-1], \"%Y\"))\n",
    "    else:\n",
    "        try:\n",
    "            time = datetime.timestamp(datetime.strptime(job[\"to\"].split(\" \")[-1], \"%Y\"))\n",
    "        except ValueError:\n",
    "            date_time_str = '2018-04-13'  # date of files creation\n",
    "            time = datetime.timestamp(datetime.strptime(date_time_str, '%Y-%m-%d'))\n",
    "    tstmp = pd.Timestamp.fromtimestamp(time)\n",
    "    return round(datetime.timestamp(tstmp.round(\"D\").to_pydatetime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edu_info(person, cie_list, syn_cie, blacklist):\n",
    "    education = person[-2]\n",
    "    jobs = []\n",
    "    flag = False\n",
    "    for job in person[1]:\n",
    "        if 'company' in job.keys():\n",
    "            threshold = min(len(job[\"company\"].split(\" \")), 5)\n",
    "            tmp = job[\"company\"].split(\" \")[:threshold]\n",
    "            normalized_name = [unidecode.unidecode(name.lower()) for name in tmp]\n",
    "            company_name = \"\".join(normalized_name)\n",
    "            if company_name in cie_list:\n",
    "                flag = True\n",
    "    if flag:\n",
    "        for job in person[1]:\n",
    "            if 'company' in job.keys():\n",
    "                threshold = min(len(job[\"company\"].split(\" \")), 5)\n",
    "                tmp = job[\"company\"].split(\" \")[:threshold]\n",
    "                normalized_name = [unidecode.unidecode(name.lower()) for name in tmp]\n",
    "                company_name = \"\".join(normalized_name)\n",
    "                if company_name not in blacklist:\n",
    "                    end = handle_date(job)\n",
    "                    tstmp = pd.Timestamp.fromtimestamp(job[\"from_ts\"])\n",
    "                    start = round(datetime.timestamp(tstmp.round(\"D\").to_pydatetime()))\n",
    "                    if company_name in syn_cie.keys():\n",
    "                        cie = syn_cie[company_name]\n",
    "                    else:\n",
    "                        cie = company_name\n",
    "                    if (end > 0) and (start > 0):  # corresponds to the timestamp of 01/01/1970\n",
    "                        j = {'from': start,\n",
    "                             'to': end,\n",
    "                             'company': cie,\n",
    "                             'job': word_seq_into_list(job[\"position\"],\n",
    "                                                       job[\"description\"], cie_list,  syn_cie)}\n",
    "                        jobs.append(j)\n",
    "\n",
    "    return education, jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 39682/850902 [01:59<42:15, 319.92it/s]  "
     ]
    }
   ],
   "source": [
    "current_file = os.path.join(CFG[\"prevdatadir\"], base_file)\n",
    "with open(current_file, 'r') as f:\n",
    "    num_lines = sum(1 for line in f)\n",
    "with open(current_file, 'r') as f:\n",
    "    pbar = tqdm(f, total=num_lines)\n",
    "    edu_backgrounds = []\n",
    "    for line in pbar:\n",
    "        try:\n",
    "            current_person = json.loads(line)\n",
    "            jobs = current_person[1]\n",
    "            skills = current_person[2]\n",
    "            if len(jobs) >= MIN_JOB_COUNT and len(skills) > 0:\n",
    "                edu_info, new_jobs = get_edu_info(current_person, cie_list, syn_cie, blacklist)\n",
    "                if len(new_jobs) >= MIN_JOB_COUNT:\n",
    "                    edu_backgrounds.extend(edu_info)\n",
    "        except OutOfBoundsDatetime:\n",
    "            continue\n",
    "        pbar.update(1)\n",
    "tgt_file = \"unprocessed_educations.pkl\"\n",
    "with open(os.path.join(CFG[\"datadir\"], tgt_file), \"wb\") as f:\n",
    "    pkl.dump(edu_backgrounds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_file = \"unprocessed_educations.pkl\"\n",
    "with open(os.path.join(CFG[\"datadir\"], tgt_file), \"wb\") as f:\n",
    "    pkl.dump(edu_backgrounds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global CFG\n",
    "with open(\"../../config.yaml\", \"r\") as ymlfile:\n",
    "    CFG = yaml.load(ymlfile, Loader=yaml.SafeLoader)\n",
    "    \n",
    "tgt_file = \"unprocessed_educations.pkl\"\n",
    "with open(os.path.join(CFG[\"datadir\"], tgt_file), \"rb\") as f:\n",
    "    data = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "accentedCharacters = \"àèìòùÀÈÌÒÙáéíóúýÁÉÍÓÚÝâêîôûÂÊÎÔÛãñõÃÑÕäëïöüÿÄËÏÖÜŸçÇßØøÅåÆæœ\"\n",
    "regex = re.compile('[^a-z0-9' + accentedCharacters + '\\s\\-]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [regex.sub(' ', i[\"degree\"].lower()) for i in data]\n",
    "institutions = [regex.sub(' ', i[\"institution\"].lower()) for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_degrees = Counter()\n",
    "count_institutions = Counter()\n",
    "for deg in degrees:\n",
    "    count_degrees[deg] +=1\n",
    "for ins in institutions:count_institutions[ins] +=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=463406, minmax=(1, 1736), mean=1.4687746813809057, variance=68.71964193230454, skewness=99.22586158959993, kurtosis=14855.39417105405)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.describe(list(count_degrees.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=143947, minmax=(1, 5210), mean=4.728400036124407, variance=2933.6574654927012, skewness=46.350406061269425, kurtosis=2889.1022316311432)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.describe(list(count_institutions.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6808396227662534"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(degrees)) / len(degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile('[a-z]')\n",
    "clean_degrees = []\n",
    "for e in sorted(list(count_degrees.keys())):\n",
    "    if regex.match(e):\n",
    "        clean_degrees.append(e.strip())\n",
    "clean_institutions = []\n",
    "for e in sorted(list(count_institutions.keys())):\n",
    "    if regex.match(e):\n",
    "        clean_institutions.append(e.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'a  cherioux',\n",
       " 'a  f  p  a champs sur marne',\n",
       " 'a  renoir',\n",
       " 'a  schweitzer - le raincy',\n",
       " 'a a a  - associations des amitiés asiatiques',\n",
       " 'a a c  académie des arts chorégraphiques cité véron  paris',\n",
       " 'a bouquinet formation',\n",
       " 'a c e',\n",
       " 'a c e p']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_institutions[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' ',\n",
       " '  ',\n",
       " '   ',\n",
       " '    ',\n",
       " '      ',\n",
       " '       ',\n",
       " '          ',\n",
       " '   annes de groupe de pratique professionnelle conseil accompagnemnt dveloppement professionnel',\n",
       " '   certificat de formation analyse transactionnelle obtenu']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(count_degrees.keys()))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../raw_degrees.txt\", 'w') as f:\n",
    "    for deg in clean_degrees:\n",
    "        f.write(deg + '\\n')\n",
    "\n",
    "with open(\"../../raw_institutions.txt\", 'w') as f:\n",
    "    for deg in clean_institutions:\n",
    "        f.write(deg + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty = \"***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'***'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(i for i in faulty if ord(i)<128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'***'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda10",
   "language": "python",
   "name": "cuda10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
